{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wine Classification with Logistic Regression\n",
        "\n",
        "This notebook demonstrates a complete machine learning workflow for classifying wine types using logistic regression.\n",
        "\n",
        "Notebook 5.1 \n",
        "\n",
        "## 1. Setup and Data Loading "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data and model libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Data manipulation and visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print('Libraries loaded successfully')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Wine Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the default wine dataset\n",
        "data = load_wine()\n",
        "print('data.keys(): ', data.keys())\n",
        "\n",
        "X = data.data       # features (independent variables)\n",
        "y = data.target     # labels (dependent variables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Overview "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Feature names:', data.feature_names)\n",
        "print('\\nTarget names:', data.target_names)\n",
        "print('\\nData shape:', data.data.shape)\n",
        "print(f'Number of samples: {data.data.shape[0]}')\n",
        "print(f'Number of features: {data.data.shape[1]}')\n",
        "print(f'Number of classes: {len(data.target_names)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Exploratory Data Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Class Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "plt.bar(data.target_names[unique], counts, color=['blue', 'red', 'black'])\n",
        "plt.title('Distribution of Wine Classes, total samples: ' + str(len(y)))\n",
        "plt.xlabel('Wine Class')\n",
        "plt.ylabel('Count')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "for i, (name, count) in enumerate(zip(data.target_names[unique], counts)):\n",
        "    plt.text(i, count + 1, str(count), ha='center')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Total samples: {len(y)}\")\n",
        "print(f\"Class distribution (%): Class 0: {100*round(counts[0]/len(y), 2):.1f}%, Class 1: {100*round(counts[1]/len(y), 2):.1f}%, Class 2: {100*round(counts[2]/len(y), 2):.1f}%\")\n",
        "print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Correlations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature correlation analysis\n",
        "# We'll create two separate visualizations:\n",
        "# 1. Feature-to-feature correlations (to check for multicollinearity)\n",
        "# 2. Feature-to-target correlations (to identify predictive features)\n",
        "\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "\n",
        "# 1. Feature-to-feature correlation matrix (excluding target)\n",
        "feature_corr_matrix = df.corr()\n",
        "\n",
        "# Plot feature-to-feature correlations (mask diagonal)\n",
        "mask = np.eye(len(feature_corr_matrix), dtype=bool)  # Mask only the diagonal\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(feature_corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='turbo', center=0,\n",
        "            square=True, linewidths=0.5, linecolor='black', cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Feature-to-Feature Correlation Matrix\\n(To identify multicollinearity)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Feature-to-target correlations\n",
        "df_with_target = df.copy()\n",
        "df_with_target['target'] = y\n",
        "target_corr = df_with_target.corr()['target'].drop('target')\n",
        "\n",
        "# Plot feature-to-target correlations\n",
        "plt.figure(figsize=(10, 8))\n",
        "target_corr_sorted = target_corr.reindex(target_corr.abs().sort_values(ascending=False).index)\n",
        "colors = ['red' if x < 0 else 'blue' for x in target_corr_sorted.values]\n",
        "bars = plt.barh(range(len(target_corr_sorted)), target_corr_sorted.values, color=colors)\n",
        "plt.yticks(range(len(target_corr_sorted)), target_corr_sorted.index)\n",
        "plt.xlabel('Correlation with Target (Wine Class)')\n",
        "plt.title('Feature-to-Target Correlations\\n(Which features are most predictive?)')\n",
        "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Add value labels\n",
        "for i, (bar, val) in enumerate(zip(bars, target_corr_sorted.values)):\n",
        "    plt.text(val + 0.01 if val > 0 else val - 0.01, i, f'{val:.3f}', \n",
        "             va='center', ha='left' if val > 0 else 'right', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nExplanation:\")\n",
        "print(\"• Feature-to-feature correlations: Check for multicollinearity (high correlations between features)\")\n",
        "print(\"• Feature-to-target correlations: Identify which features are most predictive of wine class\")\n",
        "print(\"• Target = Wine class (0, 1, or 2) - the variable we're trying to predict\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze strongest feature-to-feature correlations\n",
        "print(\"=\"*60)\n",
        "print(\"STRONGEST FEATURE-TO-FEATURE CORRELATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get upper triangle of correlation matrix (excluding diagonal)\n",
        "corr_matrix_upper = feature_corr_matrix.where(\n",
        "    np.triu(np.ones(feature_corr_matrix.shape), k=1).astype(bool)\n",
        ")\n",
        "\n",
        "# Flatten and sort correlations\n",
        "corr_pairs = []\n",
        "for i in range(len(feature_corr_matrix.columns)):\n",
        "    for j in range(i+1, len(feature_corr_matrix.columns)):\n",
        "        corr_pairs.append({\n",
        "            'Feature 1': feature_corr_matrix.columns[i],\n",
        "            'Feature 2': feature_corr_matrix.columns[j],\n",
        "            'Correlation': feature_corr_matrix.iloc[i, j]\n",
        "        })\n",
        "\n",
        "# Sort by absolute correlation value\n",
        "corr_pairs_sorted = sorted(corr_pairs, key=lambda x: abs(x['Correlation']), reverse=True)\n",
        "\n",
        "# Display top 1 strongest correlations\n",
        "print(\"\\nTop 15 Strongest Correlations (excluding self-correlations):\")\n",
        "print(\"-\"*60)\n",
        "for i, pair in enumerate(corr_pairs_sorted[:15], 1):\n",
        "    corr_val = pair['Correlation']\n",
        "    strength = \"Very Strong\" if abs(corr_val) > 0.8 else \"Strong\" if abs(corr_val) > 0.6 else \"Moderate\"\n",
        "    direction = \"Positive\" if corr_val > 0 else \"Negative\"\n",
        "    print(f\"{i}. {pair['Feature 1']} ↔ {pair['Feature 2']}\")\n",
        "    print(f\"   Correlation: {corr_val:.3f} ({direction}, {strength})\")\n",
        "    print()\n",
        "\n",
        "# Identify correlations with target variable\n",
        "print(\"\\nCorrelations with Target Variable (Wine Class):\")\n",
        "print(\"-\"*60)\n",
        "# Calculate feature-to-target correlations\n",
        "df_with_target = pd.DataFrame(X, columns=data.feature_names)\n",
        "df_with_target['target'] = y\n",
        "target_corrs = df_with_target.corr()['target'].drop('target')\n",
        "target_corrs_sorted = target_corrs.reindex(target_corrs.abs().sort_values(ascending=False).index)\n",
        "for feature, corr_val in target_corrs_sorted.items():\n",
        "    strength = \"Strong\" if abs(corr_val) > 0.5 else \"Moderate\" if abs(corr_val) > 0.3 else \"Weak\"\n",
        "    direction = \"Positive\" if corr_val > 0 else \"Negative\"\n",
        "    print(f\"{feature:25s}: {corr_val:6.3f} ({direction}, {strength})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INTERPRETATION:\")\n",
        "print(\"=\"*60)\n",
        "print(\"• High correlations (>0.7) between features indicate multicollinearity\")\n",
        "print(\"• Multicollinearity can make logistic regression coefficients unstable\")\n",
        "print(\"• Features with strong correlation to target are more predictive\")\n",
        "print(\"• Consider feature selection or regularization if multicollinearity is high\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into train (70%), validation (15%) and test (15%)sets\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
        "\n",
        "# stratify=y ensures that the proportion of each class in the train_val set is the same as in the original dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42, stratify=y_train_val)\n",
        "print('\\n')\n",
        "print(50*'-')\n",
        "print('Sanity check (proportions):')\n",
        "print(f'X_test shape: {X_test.shape[0]}/{X.shape[0]} ({100*round(X_test.shape[0]/X.shape[0], 2):.1f}%)')\n",
        "print(f'y_test shape: {y_test.shape[0]}/{y.shape[0]} ({100*round(y_test.shape[0]/y.shape[0], 2):.1f}%)')\n",
        "print(f'X_train shape: {X_train.shape[0]}/{X.shape[0]} ({100*round(X_train.shape[0]/X.shape[0], 2):.1f}%)')\n",
        "print(f'y_train shape: {y_train.shape[0]}/{y.shape[0]} ({100*round(y_train.shape[0]/y.shape[0], 2):.1f}%)')\n",
        "print(f'X_val shape: {X_val.shape[0]}/{X.shape[0]} ({100*round(X_val.shape[0]/X.shape[0], 2):.1f}%)')\n",
        "print(f'y_val shape: {y_val.shape[0]}/{y.shape[0]} ({100*round(y_val.shape[0]/y.shape[0], 2):.1f}%)')\n",
        "print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part 2: reflect on the results.\n",
        "Now that you've experimented with different data splits and observed the model's performance, reflect on your findings by answering the following questions:\n",
        "\n",
        "What impact does the 60:20:20 split have on model accuracy?\n",
        "How does the model's performance change if you use a 70:15:15 split?\n",
        "What might happen if you omitted the validation set and only used training and testing data?\n",
        "How can you apply what you've learned from experimenting with different data splits and model types to improve your capstone project's model performance and reliability?\n",
        "Completion requirements\n",
        "Submit your responses directly in the discussion board below.\n",
        "Please keep your responses under 600 words.\n",
        "Engage with peers by commenting thoughtfully on their posts.\n",
        "Consider adding these findings to your growing GitHub portfolio if you wish to showcase your work.\n",
        "The average completion time for this activity is 60 minutes.\n",
        "\n",
        "This is a self-study activity and does not count towards the completion of the programme."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit the logistic regression model\n",
        "# Logistic Regression is a linear classification algorithm that uses the logistic function\n",
        "# to model the probability of a binary or multi-class outcome.\n",
        "# \n",
        "# Parameters:\n",
        "# - max_iter=1000: Maximum number of iterations for the solver to converge\n",
        "# - random_state=42: Seed for reproducibility of results\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "# This learns the coefficients (weights) that best separate the wine classes\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Note: Model coefficients and intercepts are visualized in the Feature Importance section below\n",
        "# The coefficients show how each feature contributes to predicting each wine class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on validation set\n",
        "val_preds = model.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, val_preds)\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "test_preds = model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, test_preds)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Performance Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed classification report on test set\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Classification Report - Test Set\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_test, test_preds, target_names=data.target_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Reflection on Data Splits and Model Performance\n",
        "\n",
        "### Questions and Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. What impact does the 60:20:20 split have on model accuracy?\n",
        "\n",
        "**Analysis:**\n",
        "- **Training set (60%)**: Provides sufficient data for the model to learn patterns, but may be limited for complex models\n",
        "- **Validation set (20%)**: Allows for robust hyperparameter tuning and model selection without touching the test set\n",
        "- **Test set (20%)**: Provides a reliable estimate of generalization performance\n",
        "\n",
        "**Trade-offs:**\n",
        "- **Pros**: Good balance between training data and evaluation sets; validation set prevents overfitting to test set\n",
        "- **Cons**: Smaller training set may limit model complexity; with limited data, 60% might not capture all patterns\n",
        "\n",
        "**Impact on accuracy**: Generally provides reliable performance estimates, but may show slightly lower training accuracy due to reduced training data compared to 70:15:15 split.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. How does the model's performance change if you use a 70:15:15 split?\n",
        "\n",
        "**Analysis (Current Implementation):**\n",
        "- **Training set (70%)**: More data for learning, potentially better model fit\n",
        "- **Validation set (15%)**: Smaller but still adequate for hyperparameter tuning\n",
        "- **Test set (15%)**: Smaller test set, but sufficient for final evaluation\n",
        "\n",
        "**Observed Results:**\n",
        "- Training accuracy tends to be higher due to more training data\n",
        "- Validation accuracy provides good indication of generalization\n",
        "- Test accuracy should be close to validation accuracy if model is well-calibrated\n",
        "\n",
        "**Trade-offs:**\n",
        "- **Pros**: More training data can improve model performance, especially for complex models\n",
        "- **Cons**: Smaller validation set may provide less stable hyperparameter estimates; smaller test set has higher variance in performance estimates\n",
        "\n",
        "**Impact**: Generally shows improved training performance, but requires careful monitoring to ensure validation and test accuracies remain aligned (no overfitting).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. What might happen if you omitted the validation set and only used training and testing data?\n",
        "\n",
        "**Potential Issues:**\n",
        "\n",
        "1. **Hyperparameter Tuning Problems**:\n",
        "   - Without a validation set, hyperparameters would need to be tuned on the test set\n",
        "   - This leads to **data leakage** - the test set becomes part of model development\n",
        "   - Test set performance becomes overly optimistic and unreliable\n",
        "\n",
        "2. **Overfitting Risk**:\n",
        "   - No intermediate checkpoint to detect overfitting during training\n",
        "   - Model might memorize training data without showing signs on a separate validation set\n",
        "   - Test set becomes the only measure, but it's been \"contaminated\" by tuning\n",
        "\n",
        "3. **Model Selection Challenges**:\n",
        "   - Difficult to compare different models or architectures fairly\n",
        "   - Risk of selecting models that perform well on test set by chance (multiple testing problem)\n",
        "\n",
        "4. **Unreliable Performance Estimates**:\n",
        "   - Test accuracy may appear good but won't reflect true generalization\n",
        "   - Cannot trust the reported performance metrics\n",
        "\n",
        "**Best Practice**: Always maintain a three-way split (train/validation/test) to ensure unbiased evaluation and proper model development workflow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. How can you apply what you've learned to improve your capstone project's model performance and reliability?\n",
        "\n",
        "**Key Learnings and Applications:**\n",
        "\n",
        "1. **Proper Data Splitting Strategy**:\n",
        "   - Use stratified splits to maintain class distribution across sets\n",
        "   - Reserve test set strictly for final evaluation (never use for tuning)\n",
        "   - Choose split ratios based on dataset size: 70:15:15 for medium datasets, 80:10:10 for large datasets\n",
        "\n",
        "2. **Validation Set Importance**:\n",
        "   - Use validation set for hyperparameter tuning, model selection, and early stopping\n",
        "   - Monitor validation performance to detect overfitting early\n",
        "   - Compare validation and test accuracies to ensure model generalization\n",
        "\n",
        "3. **Feature Analysis**:\n",
        "   - Analyze feature correlations to identify multicollinearity\n",
        "   - Use feature-to-target correlations to identify most predictive features\n",
        "   - Consider feature selection or regularization if multicollinearity is high\n",
        "\n",
        "4. **Model Evaluation Best Practices**:\n",
        "   - Use multiple metrics (accuracy, precision, recall, F1-score) for comprehensive evaluation\n",
        "   - Visualize confusion matrix to understand class-specific performance\n",
        "   - Compare validation and test performance to ensure consistency\n",
        "\n",
        "5. **Reproducibility**:\n",
        "   - Always set `random_state` for reproducibility\n",
        "   - Document all hyperparameters and data splits\n",
        "   - Version control code and results\n",
        "\n",
        "**For Capstone Project**:\n",
        "- Implement robust train/validation/test splits from the start\n",
        "- Use validation set for all model development decisions\n",
        "- Keep test set untouched until final evaluation\n",
        "- Document all experiments and their results\n",
        "- Use visualization to understand model behavior and feature importance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Importance (Coefficients)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance Comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix Visualization\n",
        "# Create confusion matrix for test set\n",
        "cm = confusion_matrix(y_test, test_preds)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=data.target_names, \n",
        "            yticklabels=data.target_names)\n",
        "plt.title('Confusion Matrix - Test Set')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nTest Set Accuracy: {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Importance Visualization (Coefficients)\n",
        "# Logistic regression coefficients show the importance of each feature for each class\n",
        "\n",
        "# Get coefficients (shape: [n_classes, n_features])\n",
        "coefficients = model.coef_\n",
        "\n",
        "# Create a heatmap of coefficients\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.heatmap(coefficients, annot=True, fmt='.2f', cmap='turbo', center=0,\n",
        "            linewidths=0.5, linecolor='black', cbar_kws={\"shrink\": 0.8},\n",
        "            xticklabels=data.feature_names,\n",
        "            yticklabels=data.target_names)\n",
        "plt.title('Logistic Regression Coefficients (Feature Importance)')\n",
        "plt.ylabel('Wine Class')\n",
        "plt.xlabel('Features')\n",
        "plt.show()\n",
        "\n",
        "# Interpretation: \n",
        "# - Positive coefficients increase the probability of that class\n",
        "# - Negative coefficients decrease it\n",
        "# - Larger absolute values indicate stronger influence\n",
        "# - Each row represents one wine class, each column represents one feature\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Performance Comparison: Validation vs Test\n",
        "# Compare accuracy on validation and test sets\n",
        "\n",
        "accuracies = {\n",
        "    'Validation': val_accuracy,\n",
        "    'Test': test_accuracy\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(accuracies.keys(), accuracies.values(), color=['skyblue', 'lightcoral'])\n",
        "plt.title('Model Accuracy: Validation vs Test Set')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, (key, value) in zip(bars, accuracies.items()):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() - 0.1,\n",
        "             f'{value:.4f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "print(f\"Difference: {abs(val_accuracy - test_accuracy):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook demonstrated a complete machine learning workflow for wine classification using logistic regression:\n",
        "\n",
        "1. **Data Loading & Exploration**: Analyzed the wine dataset with 178 samples, 13 features, and 3 classes\n",
        "2. **Exploratory Data Analysis**: Visualized class distribution and feature correlations\n",
        "3. **Data Preparation**: Implemented a 70:15:15 train/validation/test split with stratification\n",
        "4. **Model Training**: Trained a logistic regression model with proper hyperparameters\n",
        "5. **Model Evaluation**: Assessed performance on both validation and test sets\n",
        "6. **Visualizations**: Created confusion matrix, feature importance heatmap, and performance comparisons\n",
        "7. **Reflection**: Analyzed the impact of different data splits and best practices\n",
        "\n",
        "**Key Takeaways:**\n",
        "- Proper data splitting (train/validation/test) is crucial for unbiased model evaluation\n",
        "- Validation set prevents data leakage and enables proper hyperparameter tuning\n",
        "- Feature analysis helps understand model behavior and identify important predictors\n",
        "- Visualization provides insights beyond numerical metrics\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
